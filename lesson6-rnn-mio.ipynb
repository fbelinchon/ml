{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-06T17:07:48.493Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from fastai.io import *\n",
    "from fastai.conv_learner import *\n",
    "\n",
    "from fastai.column_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-06T17:08:39.709Z"
    }
   },
   "outputs": [],
   "source": [
    "%system PATH\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to download the collected works of Nietzsche to use as our data for this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:46:34.710509Z",
     "start_time": "2019-01-04T18:46:34.707511Z"
    }
   },
   "outputs": [],
   "source": [
    "PATH='..\\..\\data/cervantes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:46:35.617818Z",
     "start_time": "2019-01-04T18:46:35.552855Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus length: 2065877\n"
     ]
    }
   ],
   "source": [
    "#get_data(\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\", f'{PATH}nietzsche.txt')\n",
    "text = open(f'{PATH}donquijote.txt',encoding='UTF8').read()\n",
    "print('corpus length:', len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:46:36.681833Z",
     "start_time": "2019-01-04T18:46:36.672838Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1665877"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2065877-400000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:46:38.894138Z",
     "start_time": "2019-01-04T18:46:38.889141Z"
    }
   },
   "outputs": [],
   "source": [
    "text_train=text[:1600000]\n",
    "text_test=text[1600000:]\n",
    "text_simple=text[:160000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:46:40.853234Z",
     "start_time": "2019-01-04T18:46:40.848235Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_simple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:46:42.823524Z",
     "start_time": "2019-01-04T18:46:42.816528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total chars: 75\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text_simple)))\n",
    "vocab_size = len(chars)+1\n",
    "print('total chars:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's useful to have a zero value in the dataset, e.g. for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:46:45.116703Z",
     "start_time": "2019-01-04T18:46:45.111689Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n !\"\\'(),-.:;?ABCDEFGHIJLMNOPQRSTUVXYZabcdefghijlmnopqrstuvxyz¡«»¿ÁÉá'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars.insert(0, \"\\0\")\n",
    "\n",
    "''.join(chars[1:-6])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map from chars to indices and back again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:18.852650Z",
     "start_time": "2019-01-04T18:52:18.848635Z"
    }
   },
   "outputs": [],
   "source": [
    "char_indices = {c: i for i, c in enumerate(chars)}\n",
    "indices_char = {i: c for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:19.789254Z",
     "start_time": "2019-01-04T18:52:19.783258Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '\\x00',\n",
       " 1: '\\n',\n",
       " 2: ' ',\n",
       " 3: '!',\n",
       " 4: '\"',\n",
       " 5: \"'\",\n",
       " 6: '(',\n",
       " 7: ')',\n",
       " 8: ',',\n",
       " 9: '-',\n",
       " 10: '.',\n",
       " 11: ':',\n",
       " 12: ';',\n",
       " 13: '?',\n",
       " 14: 'A',\n",
       " 15: 'B',\n",
       " 16: 'C',\n",
       " 17: 'D',\n",
       " 18: 'E',\n",
       " 19: 'F',\n",
       " 20: 'G',\n",
       " 21: 'H',\n",
       " 22: 'I',\n",
       " 23: 'J',\n",
       " 24: 'L',\n",
       " 25: 'M',\n",
       " 26: 'N',\n",
       " 27: 'O',\n",
       " 28: 'P',\n",
       " 29: 'Q',\n",
       " 30: 'R',\n",
       " 31: 'S',\n",
       " 32: 'T',\n",
       " 33: 'U',\n",
       " 34: 'V',\n",
       " 35: 'X',\n",
       " 36: 'Y',\n",
       " 37: 'Z',\n",
       " 38: 'a',\n",
       " 39: 'b',\n",
       " 40: 'c',\n",
       " 41: 'd',\n",
       " 42: 'e',\n",
       " 43: 'f',\n",
       " 44: 'g',\n",
       " 45: 'h',\n",
       " 46: 'i',\n",
       " 47: 'j',\n",
       " 48: 'l',\n",
       " 49: 'm',\n",
       " 50: 'n',\n",
       " 51: 'o',\n",
       " 52: 'p',\n",
       " 53: 'q',\n",
       " 54: 'r',\n",
       " 55: 's',\n",
       " 56: 't',\n",
       " 57: 'u',\n",
       " 58: 'v',\n",
       " 59: 'x',\n",
       " 60: 'y',\n",
       " 61: 'z',\n",
       " 62: '¡',\n",
       " 63: '«',\n",
       " 64: '»',\n",
       " 65: '¿',\n",
       " 66: 'Á',\n",
       " 67: 'É',\n",
       " 68: 'á',\n",
       " 69: 'é',\n",
       " 70: 'í',\n",
       " 71: 'ñ',\n",
       " 72: 'ó',\n",
       " 73: 'ú',\n",
       " 74: 'ü'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*idx* will be the data we use from now on - it simply converts all the characters to their index (based on the mapping above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:21.645426Z",
     "start_time": "2019-01-04T18:52:21.630418Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 28, 54, 46, 49, 42, 54, 38, 2, 52]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = [char_indices[c] for c in text_simple]\n",
    "\n",
    "idx[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:22.532601Z",
     "start_time": "2019-01-04T18:52:22.527606Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Primera parte del ingenioso hidalgo don Quijote de la Mancha   Capítu'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(indices_char[i] for i in idx[:70])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three char model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of every 4th character, starting at the 0th, 1st, 2nd, then 3rd characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c1_data = a los caracteres 1,4,7,10 ..... (empieza en 1 y va de 3 en 3)\n",
    "<br>\n",
    "c2_data = a los caracteres en posición 2,5,8 ....\n",
    "<br>\n",
    "c3_data = a las posiciones 3,6,9 ...\n",
    "<br>\n",
    "c4_data = a las posiciones 4,7,10..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:25.996706Z",
     "start_time": "2019-01-04T18:52:25.974720Z"
    }
   },
   "outputs": [],
   "source": [
    "cs=3\n",
    "c1_dat = [idx[i]   for i in range(0, len(idx)-cs, cs)]\n",
    "c2_dat = [idx[i+1] for i in range(0, len(idx)-cs, cs)]\n",
    "c3_dat = [idx[i+2] for i in range(0, len(idx)-cs, cs)]\n",
    "c4_dat = [idx[i+3] for i in range(0, len(idx)-cs, cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:26.874729Z",
     "start_time": "2019-01-04T18:52:26.864735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[46,\n",
       " 54,\n",
       " 52,\n",
       " 56,\n",
       " 41,\n",
       " 2,\n",
       " 44,\n",
       " 46,\n",
       " 51,\n",
       " 46,\n",
       " 48,\n",
       " 2,\n",
       " 50,\n",
       " 57,\n",
       " 51,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 50,\n",
       " 38,\n",
       " 2,\n",
       " 52,\n",
       " 57,\n",
       " 2,\n",
       " 46,\n",
       " 54,\n",
       " 2,\n",
       " 42,\n",
       " 54,\n",
       " 38,\n",
       " 42,\n",
       " 38,\n",
       " 51,\n",
       " 46,\n",
       " 72,\n",
       " 60,\n",
       " 47,\n",
       " 40,\n",
       " 46,\n",
       " 41,\n",
       " 2,\n",
       " 49,\n",
       " 51,\n",
       " 46,\n",
       " 48,\n",
       " 1,\n",
       " 50,\n",
       " 57,\n",
       " 51,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 50,\n",
       " 38,\n",
       " 18,\n",
       " 57,\n",
       " 48,\n",
       " 38,\n",
       " 41,\n",
       " 48,\n",
       " 25,\n",
       " 40,\n",
       " 8,\n",
       " 42,\n",
       " 57,\n",
       " 2,\n",
       " 49,\n",
       " 42,\n",
       " 51,\n",
       " 57,\n",
       " 54,\n",
       " 38,\n",
       " 54,\n",
       " 54,\n",
       " 8,\n",
       " 51,\n",
       " 38,\n",
       " 57,\n",
       " 51,\n",
       " 46,\n",
       " 52,\n",
       " 53,\n",
       " 2,\n",
       " 58,\n",
       " 2,\n",
       " 2,\n",
       " 41,\n",
       " 44,\n",
       " 41,\n",
       " 48,\n",
       " 2,\n",
       " 2,\n",
       " 50,\n",
       " 2,\n",
       " 2,\n",
       " 56,\n",
       " 48,\n",
       " 51,\n",
       " 38,\n",
       " 54,\n",
       " 2,\n",
       " 56,\n",
       " 57,\n",
       " 1,\n",
       " 40,\n",
       " 2,\n",
       " 38,\n",
       " 2,\n",
       " 44,\n",
       " 44,\n",
       " 40,\n",
       " 54,\n",
       " 51,\n",
       " 2,\n",
       " 38,\n",
       " 48,\n",
       " 2,\n",
       " 2,\n",
       " 44,\n",
       " 49,\n",
       " 2,\n",
       " 40,\n",
       " 53,\n",
       " 2,\n",
       " 54,\n",
       " 54,\n",
       " 1,\n",
       " 48,\n",
       " 40,\n",
       " 2,\n",
       " 55,\n",
       " 68,\n",
       " 50,\n",
       " 45,\n",
       " 8,\n",
       " 57,\n",
       " 51,\n",
       " 60,\n",
       " 57,\n",
       " 54,\n",
       " 56,\n",
       " 2,\n",
       " 55,\n",
       " 68,\n",
       " 41,\n",
       " 8,\n",
       " 38,\n",
       " 42,\n",
       " 55,\n",
       " 51,\n",
       " 58,\n",
       " 54,\n",
       " 55,\n",
       " 38,\n",
       " 73,\n",
       " 52,\n",
       " 51,\n",
       " 50,\n",
       " 41,\n",
       " 38,\n",
       " 41,\n",
       " 57,\n",
       " 2,\n",
       " 55,\n",
       " 51,\n",
       " 50,\n",
       " 55,\n",
       " 40,\n",
       " 55,\n",
       " 70,\n",
       " 2,\n",
       " 55,\n",
       " 54,\n",
       " 1,\n",
       " 54,\n",
       " 55,\n",
       " 42,\n",
       " 57,\n",
       " 38,\n",
       " 42,\n",
       " 38,\n",
       " 18,\n",
       " 54,\n",
       " 56,\n",
       " 41,\n",
       " 48,\n",
       " 40,\n",
       " 40,\n",
       " 70,\n",
       " 2,\n",
       " 60,\n",
       " 41,\n",
       " 58,\n",
       " 38,\n",
       " 42,\n",
       " 40,\n",
       " 61,\n",
       " 2,\n",
       " 1,\n",
       " 48,\n",
       " 41,\n",
       " 52,\n",
       " 38,\n",
       " 38,\n",
       " 43,\n",
       " 55,\n",
       " 55,\n",
       " 40,\n",
       " 2,\n",
       " 55,\n",
       " 38,\n",
       " 57,\n",
       " 51,\n",
       " 41,\n",
       " 48,\n",
       " 49,\n",
       " 49,\n",
       " 2,\n",
       " 48,\n",
       " 2,\n",
       " 38,\n",
       " 41,\n",
       " 42,\n",
       " 54,\n",
       " 42,\n",
       " 50,\n",
       " 55,\n",
       " 45,\n",
       " 54,\n",
       " 38,\n",
       " 51,\n",
       " 55,\n",
       " 58,\n",
       " 48,\n",
       " 70,\n",
       " 42,\n",
       " 51,\n",
       " 68,\n",
       " 43,\n",
       " 51,\n",
       " 32,\n",
       " 70,\n",
       " 42,\n",
       " 55,\n",
       " 40,\n",
       " 38,\n",
       " 50,\n",
       " 38,\n",
       " 2,\n",
       " 42,\n",
       " 38,\n",
       " 39,\n",
       " 41,\n",
       " 48,\n",
       " 2,\n",
       " 38,\n",
       " 50,\n",
       " 8,\n",
       " 2,\n",
       " 38,\n",
       " 51,\n",
       " 46,\n",
       " 2,\n",
       " 42,\n",
       " 51,\n",
       " 48,\n",
       " 38,\n",
       " 2,\n",
       " 48,\n",
       " 2,\n",
       " 46,\n",
       " 42,\n",
       " 60,\n",
       " 50,\n",
       " 51,\n",
       " 2,\n",
       " 2,\n",
       " 49,\n",
       " 2,\n",
       " 52,\n",
       " 61,\n",
       " 2,\n",
       " 42,\n",
       " 55,\n",
       " 42,\n",
       " 46,\n",
       " 38,\n",
       " 2,\n",
       " 2,\n",
       " 40,\n",
       " 2,\n",
       " 49,\n",
       " 56,\n",
       " 38,\n",
       " 2,\n",
       " 1,\n",
       " 41,\n",
       " 42,\n",
       " 10,\n",
       " 54,\n",
       " 38,\n",
       " 2,\n",
       " 2,\n",
       " 38,\n",
       " 41,\n",
       " 50,\n",
       " 55,\n",
       " 51,\n",
       " 46,\n",
       " 48,\n",
       " 2,\n",
       " 50,\n",
       " 51,\n",
       " 40,\n",
       " 40,\n",
       " 50,\n",
       " 2,\n",
       " 51,\n",
       " 2,\n",
       " 38,\n",
       " 42,\n",
       " 51,\n",
       " 48,\n",
       " 46,\n",
       " 2,\n",
       " 40,\n",
       " 8,\n",
       " 42,\n",
       " 2,\n",
       " 2,\n",
       " 54,\n",
       " 55,\n",
       " 42,\n",
       " 57,\n",
       " 2,\n",
       " 2,\n",
       " 55,\n",
       " 51,\n",
       " 44,\n",
       " 50,\n",
       " 38,\n",
       " 57,\n",
       " 41,\n",
       " 2,\n",
       " 38,\n",
       " 44,\n",
       " 41,\n",
       " 48,\n",
       " 40,\n",
       " 38,\n",
       " 29,\n",
       " 42,\n",
       " 50,\n",
       " 42,\n",
       " 54,\n",
       " 57,\n",
       " 56,\n",
       " 70,\n",
       " 42,\n",
       " 55,\n",
       " 54,\n",
       " 51,\n",
       " 54,\n",
       " 41,\n",
       " 29,\n",
       " 47,\n",
       " 38,\n",
       " 51,\n",
       " 57,\n",
       " 38,\n",
       " 8,\n",
       " 57,\n",
       " 42,\n",
       " 42,\n",
       " 51,\n",
       " 38,\n",
       " 38,\n",
       " 57,\n",
       " 2,\n",
       " 43,\n",
       " 42,\n",
       " 46,\n",
       " 42,\n",
       " 48,\n",
       " 2,\n",
       " 56,\n",
       " 42,\n",
       " 53,\n",
       " 2,\n",
       " 55,\n",
       " 2,\n",
       " 55,\n",
       " 42,\n",
       " 54,\n",
       " 42,\n",
       " 1,\n",
       " 50,\n",
       " 42,\n",
       " 52,\n",
       " 2,\n",
       " 50,\n",
       " 56,\n",
       " 38,\n",
       " 58,\n",
       " 51,\n",
       " 49,\n",
       " 42,\n",
       " 2,\n",
       " 2,\n",
       " 47,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 53,\n",
       " 2,\n",
       " 2,\n",
       " 38,\n",
       " 39,\n",
       " 29,\n",
       " 47,\n",
       " 38,\n",
       " 28,\n",
       " 51,\n",
       " 55,\n",
       " 2,\n",
       " 52,\n",
       " 56,\n",
       " 52,\n",
       " 51,\n",
       " 2,\n",
       " 42,\n",
       " 54,\n",
       " 40,\n",
       " 50,\n",
       " 12,\n",
       " 38,\n",
       " 38,\n",
       " 57,\n",
       " 42,\n",
       " 48,\n",
       " 50,\n",
       " 54,\n",
       " 46,\n",
       " 1,\n",
       " 48,\n",
       " 51,\n",
       " 42,\n",
       " 38,\n",
       " 38,\n",
       " 50,\n",
       " 57,\n",
       " 51,\n",
       " 42,\n",
       " 38,\n",
       " 42,\n",
       " 38,\n",
       " 2,\n",
       " 8,\n",
       " 57,\n",
       " 8,\n",
       " 42,\n",
       " 38,\n",
       " 54,\n",
       " 57,\n",
       " 42,\n",
       " 42,\n",
       " 51,\n",
       " 42,\n",
       " 40,\n",
       " 2,\n",
       " 41,\n",
       " 44,\n",
       " 2,\n",
       " 55,\n",
       " 38,\n",
       " 55,\n",
       " 57,\n",
       " 42,\n",
       " 38,\n",
       " 1,\n",
       " 46,\n",
       " 51,\n",
       " 53,\n",
       " 2,\n",
       " 38,\n",
       " 48,\n",
       " 2,\n",
       " 55,\n",
       " 42,\n",
       " 38,\n",
       " 8,\n",
       " 42,\n",
       " 38,\n",
       " 2,\n",
       " 48,\n",
       " 54,\n",
       " 46,\n",
       " 51,\n",
       " 41,\n",
       " 40,\n",
       " 38,\n",
       " 42,\n",
       " 38,\n",
       " 2,\n",
       " 50,\n",
       " 38,\n",
       " 38,\n",
       " 43,\n",
       " 46,\n",
       " 2,\n",
       " 44,\n",
       " 56,\n",
       " 2,\n",
       " 42,\n",
       " 48,\n",
       " 41,\n",
       " 40,\n",
       " 46,\n",
       " 42,\n",
       " 51,\n",
       " 2,\n",
       " 50,\n",
       " 2,\n",
       " 2,\n",
       " 42,\n",
       " 46,\n",
       " 51,\n",
       " 42,\n",
       " 38,\n",
       " 38,\n",
       " 8,\n",
       " 2,\n",
       " 50,\n",
       " 38,\n",
       " 41,\n",
       " 50,\n",
       " 56,\n",
       " 40,\n",
       " 50,\n",
       " 42,\n",
       " 57,\n",
       " 38,\n",
       " 42,\n",
       " 38,\n",
       " 36,\n",
       " 48,\n",
       " 72,\n",
       " 2,\n",
       " 50,\n",
       " 2,\n",
       " 2,\n",
       " 54,\n",
       " 55,\n",
       " 38,\n",
       " 60,\n",
       " 42,\n",
       " 56,\n",
       " 51,\n",
       " 50,\n",
       " 55,\n",
       " 8,\n",
       " 57,\n",
       " 58,\n",
       " 41,\n",
       " 2,\n",
       " 40,\n",
       " 55,\n",
       " 38,\n",
       " 44,\n",
       " 2,\n",
       " 2,\n",
       " 42,\n",
       " 38,\n",
       " 42,\n",
       " 42,\n",
       " 54,\n",
       " 57,\n",
       " 2,\n",
       " 54,\n",
       " 40,\n",
       " 52,\n",
       " 54,\n",
       " 46,\n",
       " 51,\n",
       " 41,\n",
       " 40,\n",
       " 38,\n",
       " 42,\n",
       " 38,\n",
       " 42,\n",
       " 53,\n",
       " 2,\n",
       " 42,\n",
       " 2,\n",
       " 38,\n",
       " 8,\n",
       " 48,\n",
       " 72,\n",
       " 2,\n",
       " 2,\n",
       " 55,\n",
       " 56,\n",
       " 51,\n",
       " 40,\n",
       " 50,\n",
       " 55,\n",
       " 57,\n",
       " 2,\n",
       " 39,\n",
       " 2,\n",
       " 48,\n",
       " 55,\n",
       " 60,\n",
       " 42,\n",
       " 51,\n",
       " 55,\n",
       " 50,\n",
       " 44,\n",
       " 51,\n",
       " 48,\n",
       " 52,\n",
       " 42,\n",
       " 38,\n",
       " 56,\n",
       " 2,\n",
       " 42,\n",
       " 40,\n",
       " 51,\n",
       " 51,\n",
       " 53,\n",
       " 2,\n",
       " 49,\n",
       " 55,\n",
       " 42,\n",
       " 43,\n",
       " 51,\n",
       " 2,\n",
       " 48,\n",
       " 46,\n",
       " 51,\n",
       " 42,\n",
       " 46,\n",
       " 38,\n",
       " 52,\n",
       " 53,\n",
       " 2,\n",
       " 2,\n",
       " 38,\n",
       " 41,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 51,\n",
       " 2,\n",
       " 38,\n",
       " 42,\n",
       " 38,\n",
       " 42,\n",
       " 54,\n",
       " 38,\n",
       " 55,\n",
       " 38,\n",
       " 50,\n",
       " 2,\n",
       " 60,\n",
       " 2,\n",
       " 2,\n",
       " 54,\n",
       " 70,\n",
       " 2,\n",
       " 2,\n",
       " 54,\n",
       " 55,\n",
       " 60,\n",
       " 68,\n",
       " 40,\n",
       " 50,\n",
       " 2,\n",
       " 42,\n",
       " 39,\n",
       " 38,\n",
       " 42,\n",
       " 2,\n",
       " 57,\n",
       " 48,\n",
       " 2,\n",
       " 53,\n",
       " 42,\n",
       " 51,\n",
       " 60,\n",
       " 38,\n",
       " 38,\n",
       " 41,\n",
       " 41,\n",
       " 38,\n",
       " 51,\n",
       " 2,\n",
       " 50,\n",
       " 2,\n",
       " 1,\n",
       " 40,\n",
       " 55,\n",
       " 38,\n",
       " 42,\n",
       " 45,\n",
       " 48,\n",
       " 38,\n",
       " 55,\n",
       " 46,\n",
       " 11,\n",
       " 38,\n",
       " 38,\n",
       " 50,\n",
       " 42,\n",
       " 38,\n",
       " 46,\n",
       " 38,\n",
       " 50,\n",
       " 57,\n",
       " 38,\n",
       " 46,\n",
       " 38,\n",
       " 50,\n",
       " 42,\n",
       " 38,\n",
       " 8,\n",
       " 42,\n",
       " 38,\n",
       " 49,\n",
       " 42,\n",
       " 2,\n",
       " 2,\n",
       " 61,\n",
       " 2,\n",
       " 43,\n",
       " 53,\n",
       " 40,\n",
       " 2,\n",
       " 42,\n",
       " 51,\n",
       " 54,\n",
       " 72,\n",
       " 49,\n",
       " 53,\n",
       " 47,\n",
       " 41,\n",
       " 48,\n",
       " 58,\n",
       " 55,\n",
       " 38,\n",
       " 42,\n",
       " 51,\n",
       " 54,\n",
       " 2,\n",
       " 56,\n",
       " 39,\n",
       " 50,\n",
       " 57,\n",
       " 41,\n",
       " 48,\n",
       " 38,\n",
       " 10,\n",
       " 48,\n",
       " 2,\n",
       " 56,\n",
       " 2,\n",
       " 42,\n",
       " 55,\n",
       " 57,\n",
       " 41,\n",
       " 58,\n",
       " 55,\n",
       " 38,\n",
       " 46,\n",
       " 50,\n",
       " 38,\n",
       " 41,\n",
       " 46,\n",
       " 49,\n",
       " 56,\n",
       " 40,\n",
       " 2,\n",
       " 55,\n",
       " 55,\n",
       " 42,\n",
       " 38,\n",
       " 51,\n",
       " 43,\n",
       " 56,\n",
       " 46,\n",
       " 50,\n",
       " 60,\n",
       " 55,\n",
       " 38,\n",
       " 50,\n",
       " 42,\n",
       " 40,\n",
       " 51,\n",
       " 2,\n",
       " 48,\n",
       " 42,\n",
       " 40,\n",
       " 46,\n",
       " 56,\n",
       " 53,\n",
       " 2,\n",
       " 54,\n",
       " 42,\n",
       " 38,\n",
       " 57,\n",
       " 56,\n",
       " 2,\n",
       " 38,\n",
       " 42,\n",
       " 10,\n",
       " 51,\n",
       " 42,\n",
       " 38,\n",
       " 54,\n",
       " 51,\n",
       " 55,\n",
       " 42,\n",
       " 70,\n",
       " 42,\n",
       " 52,\n",
       " 54,\n",
       " 40,\n",
       " 38,\n",
       " 42,\n",
       " 2,\n",
       " 2,\n",
       " 46,\n",
       " 51,\n",
       " 60,\n",
       " 42,\n",
       " 42,\n",
       " 39,\n",
       " 42,\n",
       " 51,\n",
       " 42,\n",
       " 42,\n",
       " 42,\n",
       " 38,\n",
       " 60,\n",
       " 42,\n",
       " 50,\n",
       " 38,\n",
       " 54,\n",
       " 55,\n",
       " 48,\n",
       " 42,\n",
       " 46,\n",
       " 8,\n",
       " 57,\n",
       " 50,\n",
       " 55,\n",
       " 48,\n",
       " 55,\n",
       " 38,\n",
       " 2,\n",
       " 2,\n",
       " 55,\n",
       " 50,\n",
       " 50,\n",
       " 42,\n",
       " 2,\n",
       " 2,\n",
       " 55,\n",
       " 2,\n",
       " 46,\n",
       " 72,\n",
       " 48,\n",
       " 8,\n",
       " 46,\n",
       " 42,\n",
       " 40,\n",
       " 38,\n",
       " 2,\n",
       " 54,\n",
       " 55,\n",
       " 51,\n",
       " 48,\n",
       " 10,\n",
       " 51,\n",
       " 55,\n",
       " 39,\n",
       " 49,\n",
       " 2,\n",
       " 42,\n",
       " 40,\n",
       " 2,\n",
       " 55,\n",
       " 42,\n",
       " 41,\n",
       " 2,\n",
       " 42,\n",
       " 51,\n",
       " 15,\n",
       " 46,\n",
       " 70,\n",
       " 41,\n",
       " 38,\n",
       " 2,\n",
       " 40,\n",
       " 70,\n",
       " 2,\n",
       " 54,\n",
       " 42,\n",
       " 42,\n",
       " 49,\n",
       " 46,\n",
       " 39,\n",
       " 53,\n",
       " 8,\n",
       " 51,\n",
       " 44,\n",
       " 50,\n",
       " 55,\n",
       " 38,\n",
       " 56,\n",
       " 55,\n",
       " 57,\n",
       " 48,\n",
       " 45,\n",
       " 46,\n",
       " 42,\n",
       " 40,\n",
       " 38,\n",
       " 8,\n",
       " 51,\n",
       " 42,\n",
       " 54,\n",
       " 2,\n",
       " 1,\n",
       " 50,\n",
       " 2,\n",
       " 2,\n",
       " 55,\n",
       " 51,\n",
       " 2,\n",
       " 41,\n",
       " 42,\n",
       " 40,\n",
       " 54,\n",
       " 2,\n",
       " 42,\n",
       " 2,\n",
       " 2,\n",
       " 40,\n",
       " 54,\n",
       " 42,\n",
       " 60,\n",
       " 42,\n",
       " 48,\n",
       " 10,\n",
       " 42,\n",
       " 8,\n",
       " 51,\n",
       " 56,\n",
       " 51,\n",
       " 38,\n",
       " 39,\n",
       " 38,\n",
       " 50,\n",
       " 57,\n",
       " 57,\n",
       " 54,\n",
       " 53,\n",
       " 48,\n",
       " 40,\n",
       " 38,\n",
       " 55,\n",
       " 48,\n",
       " 54,\n",
       " 40,\n",
       " 2,\n",
       " 2,\n",
       " 51,\n",
       " 55,\n",
       " 41,\n",
       " 38,\n",
       " 42,\n",
       " 38,\n",
       " 50,\n",
       " 38,\n",
       " 39,\n",
       " 2,\n",
       " 42,\n",
       " 57,\n",
       " 8,\n",
       " 2,\n",
       " 40,\n",
       " 55,\n",
       " 42,\n",
       " 55,\n",
       " 42,\n",
       " 46,\n",
       " 2,\n",
       " 55,\n",
       " 2,\n",
       " ...]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c4_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:27.868133Z",
     "start_time": "2019-01-04T18:52:27.856139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[28,\n",
       " 49,\n",
       " 38,\n",
       " 38,\n",
       " 42,\n",
       " 42,\n",
       " 46,\n",
       " 42,\n",
       " 51,\n",
       " 2,\n",
       " 41,\n",
       " 44,\n",
       " 41,\n",
       " 2,\n",
       " 46,\n",
       " 56,\n",
       " 41,\n",
       " 48,\n",
       " 25,\n",
       " 40,\n",
       " 2,\n",
       " 16,\n",
       " 70,\n",
       " 48,\n",
       " 52,\n",
       " 49,\n",
       " 51,\n",
       " 29,\n",
       " 2,\n",
       " 38,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 50,\n",
       " 40,\n",
       " 50,\n",
       " 2,\n",
       " 42,\n",
       " 46,\n",
       " 51,\n",
       " 42,\n",
       " 43,\n",
       " 51,\n",
       " 2,\n",
       " 41,\n",
       " 44,\n",
       " 41,\n",
       " 2,\n",
       " 46,\n",
       " 56,\n",
       " 41,\n",
       " 48,\n",
       " 25,\n",
       " 40,\n",
       " 2,\n",
       " 50,\n",
       " 50,\n",
       " 57,\n",
       " 54,\n",
       " 42,\n",
       " 38,\n",
       " 38,\n",
       " 45,\n",
       " 2,\n",
       " 2,\n",
       " 60,\n",
       " 50,\n",
       " 39,\n",
       " 2,\n",
       " 2,\n",
       " 46,\n",
       " 51,\n",
       " 40,\n",
       " 41,\n",
       " 49,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 40,\n",
       " 1,\n",
       " 42,\n",
       " 51,\n",
       " 57,\n",
       " 58,\n",
       " 70,\n",
       " 57,\n",
       " 45,\n",
       " 38,\n",
       " 51,\n",
       " 42,\n",
       " 51,\n",
       " 41,\n",
       " 48,\n",
       " 61,\n",
       " 42,\n",
       " 38,\n",
       " 46,\n",
       " 42,\n",
       " 8,\n",
       " 41,\n",
       " 44,\n",
       " 38,\n",
       " 46,\n",
       " 38,\n",
       " 54,\n",
       " 70,\n",
       " 43,\n",
       " 40,\n",
       " 60,\n",
       " 38,\n",
       " 51,\n",
       " 51,\n",
       " 42,\n",
       " 54,\n",
       " 33,\n",
       " 2,\n",
       " 48,\n",
       " 41,\n",
       " 38,\n",
       " 51,\n",
       " 68,\n",
       " 58,\n",
       " 38,\n",
       " 57,\n",
       " 40,\n",
       " 50,\n",
       " 51,\n",
       " 55,\n",
       " 52,\n",
       " 72,\n",
       " 48,\n",
       " 2,\n",
       " 55,\n",
       " 51,\n",
       " 42,\n",
       " 2,\n",
       " 42,\n",
       " 55,\n",
       " 2,\n",
       " 42,\n",
       " 38,\n",
       " 51,\n",
       " 48,\n",
       " 2,\n",
       " 39,\n",
       " 51,\n",
       " 2,\n",
       " 50,\n",
       " 47,\n",
       " 2,\n",
       " 55,\n",
       " 46,\n",
       " 50,\n",
       " 8,\n",
       " 48,\n",
       " 50,\n",
       " 38,\n",
       " 49,\n",
       " 51,\n",
       " 42,\n",
       " 71,\n",
       " 46,\n",
       " 54,\n",
       " 48,\n",
       " 2,\n",
       " 49,\n",
       " 44,\n",
       " 8,\n",
       " 51,\n",
       " 57,\n",
       " 38,\n",
       " 48,\n",
       " 2,\n",
       " 42,\n",
       " 52,\n",
       " 56,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 40,\n",
       " 50,\n",
       " 10,\n",
       " 48,\n",
       " 42,\n",
       " 51,\n",
       " 42,\n",
       " 38,\n",
       " 51,\n",
       " 48,\n",
       " 38,\n",
       " 55,\n",
       " 51,\n",
       " 42,\n",
       " 42,\n",
       " 54,\n",
       " 8,\n",
       " 38,\n",
       " 38,\n",
       " 41,\n",
       " 58,\n",
       " 48,\n",
       " 51,\n",
       " 38,\n",
       " 2,\n",
       " 55,\n",
       " 46,\n",
       " 56,\n",
       " 8,\n",
       " 51,\n",
       " 55,\n",
       " 2,\n",
       " 50,\n",
       " 43,\n",
       " 55,\n",
       " 42,\n",
       " 51,\n",
       " 42,\n",
       " 51,\n",
       " 60,\n",
       " 51,\n",
       " 41,\n",
       " 55,\n",
       " 42,\n",
       " 50,\n",
       " 42,\n",
       " 49,\n",
       " 38,\n",
       " 42,\n",
       " 51,\n",
       " 38,\n",
       " 2,\n",
       " 50,\n",
       " 57,\n",
       " 42,\n",
       " 51,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 55,\n",
       " 46,\n",
       " 10,\n",
       " 42,\n",
       " 38,\n",
       " 50,\n",
       " 57,\n",
       " 38,\n",
       " 2,\n",
       " 38,\n",
       " 49,\n",
       " 53,\n",
       " 2,\n",
       " 55,\n",
       " 38,\n",
       " 42,\n",
       " 51,\n",
       " 40,\n",
       " 54,\n",
       " 56,\n",
       " 2,\n",
       " 57,\n",
       " 2,\n",
       " 39,\n",
       " 50,\n",
       " 53,\n",
       " 2,\n",
       " 2,\n",
       " 42,\n",
       " 39,\n",
       " 38,\n",
       " 51,\n",
       " 58,\n",
       " 50,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 61,\n",
       " 41,\n",
       " 40,\n",
       " 52,\n",
       " 60,\n",
       " 48,\n",
       " 38,\n",
       " 53,\n",
       " 2,\n",
       " 70,\n",
       " 50,\n",
       " 48,\n",
       " 39,\n",
       " 42,\n",
       " 54,\n",
       " 70,\n",
       " 40,\n",
       " 51,\n",
       " 51,\n",
       " 39,\n",
       " 48,\n",
       " 52,\n",
       " 38,\n",
       " 54,\n",
       " 2,\n",
       " 46,\n",
       " 39,\n",
       " 48,\n",
       " 42,\n",
       " 41,\n",
       " 42,\n",
       " 57,\n",
       " 56,\n",
       " 2,\n",
       " 41,\n",
       " 44,\n",
       " 40,\n",
       " 2,\n",
       " 55,\n",
       " 46,\n",
       " 57,\n",
       " 56,\n",
       " 38,\n",
       " 55,\n",
       " 42,\n",
       " 2,\n",
       " 1,\n",
       " 49,\n",
       " 42,\n",
       " 72,\n",
       " 54,\n",
       " 46,\n",
       " 2,\n",
       " 40,\n",
       " 41,\n",
       " 40,\n",
       " 50,\n",
       " 8,\n",
       " 50,\n",
       " 56,\n",
       " 41,\n",
       " 54,\n",
       " 56,\n",
       " 8,\n",
       " 54,\n",
       " 2,\n",
       " 41,\n",
       " 44,\n",
       " 51,\n",
       " 60,\n",
       " 49,\n",
       " 51,\n",
       " 42,\n",
       " 38,\n",
       " 38,\n",
       " 10,\n",
       " 57,\n",
       " 54,\n",
       " 2,\n",
       " 40,\n",
       " 2,\n",
       " 42,\n",
       " 42,\n",
       " 38,\n",
       " 48,\n",
       " 51,\n",
       " 42,\n",
       " 49,\n",
       " 42,\n",
       " 42,\n",
       " 57,\n",
       " 38,\n",
       " 8,\n",
       " 2,\n",
       " 42,\n",
       " 41,\n",
       " 1,\n",
       " 42,\n",
       " 50,\n",
       " 55,\n",
       " 2,\n",
       " 60,\n",
       " 48,\n",
       " 50,\n",
       " 41,\n",
       " 42,\n",
       " 50,\n",
       " 38,\n",
       " 50,\n",
       " 51,\n",
       " 38,\n",
       " 51,\n",
       " 55,\n",
       " 57,\n",
       " 41,\n",
       " 56,\n",
       " 40,\n",
       " 51,\n",
       " 55,\n",
       " 46,\n",
       " 50,\n",
       " 38,\n",
       " 53,\n",
       " 8,\n",
       " 51,\n",
       " 40,\n",
       " 47,\n",
       " 57,\n",
       " 55,\n",
       " 42,\n",
       " 55,\n",
       " 46,\n",
       " 55,\n",
       " 55,\n",
       " 41,\n",
       " 38,\n",
       " 50,\n",
       " 50,\n",
       " 54,\n",
       " 57,\n",
       " 55,\n",
       " 48,\n",
       " 49,\n",
       " 38,\n",
       " 57,\n",
       " 38,\n",
       " 10,\n",
       " 42,\n",
       " 2,\n",
       " 56,\n",
       " 46,\n",
       " 51,\n",
       " 38,\n",
       " 51,\n",
       " 2,\n",
       " 50,\n",
       " 55,\n",
       " 51,\n",
       " 57,\n",
       " 56,\n",
       " 2,\n",
       " 55,\n",
       " 2,\n",
       " 42,\n",
       " 50,\n",
       " 38,\n",
       " 38,\n",
       " 38,\n",
       " 72,\n",
       " 41,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 48,\n",
       " 2,\n",
       " 2,\n",
       " 50,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 54,\n",
       " 41,\n",
       " 18,\n",
       " 2,\n",
       " 42,\n",
       " 2,\n",
       " 2,\n",
       " 39,\n",
       " 2,\n",
       " 42,\n",
       " 55,\n",
       " 2,\n",
       " 39,\n",
       " 41,\n",
       " 45,\n",
       " 45,\n",
       " 38,\n",
       " 51,\n",
       " 48,\n",
       " 2,\n",
       " 56,\n",
       " 2,\n",
       " 42,\n",
       " 55,\n",
       " 39,\n",
       " 51,\n",
       " 51,\n",
       " 8,\n",
       " 57,\n",
       " 42,\n",
       " 50,\n",
       " 51,\n",
       " 49,\n",
       " 2,\n",
       " 48,\n",
       " 71,\n",
       " 2,\n",
       " 2,\n",
       " 39,\n",
       " 38,\n",
       " 42,\n",
       " 2,\n",
       " 39,\n",
       " 55,\n",
       " 42,\n",
       " 38,\n",
       " 48,\n",
       " 54,\n",
       " 55,\n",
       " 40,\n",
       " 1,\n",
       " 50,\n",
       " 2,\n",
       " 46,\n",
       " 72,\n",
       " 60,\n",
       " 57,\n",
       " 51,\n",
       " 53,\n",
       " 2,\n",
       " 58,\n",
       " 72,\n",
       " 38,\n",
       " 2,\n",
       " 2,\n",
       " 41,\n",
       " 52,\n",
       " 56,\n",
       " 42,\n",
       " 42,\n",
       " 54,\n",
       " 40,\n",
       " 2,\n",
       " 2,\n",
       " 1,\n",
       " 61,\n",
       " 2,\n",
       " 38,\n",
       " 2,\n",
       " 2,\n",
       " 49,\n",
       " 46,\n",
       " 54,\n",
       " 46,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 40,\n",
       " 50,\n",
       " 10,\n",
       " 2,\n",
       " 42,\n",
       " 2,\n",
       " 56,\n",
       " 56,\n",
       " 55,\n",
       " 40,\n",
       " 46,\n",
       " 46,\n",
       " 41,\n",
       " 2,\n",
       " 55,\n",
       " 46,\n",
       " 2,\n",
       " 2,\n",
       " 56,\n",
       " 2,\n",
       " 42,\n",
       " 42,\n",
       " 46,\n",
       " 49,\n",
       " 45,\n",
       " 2,\n",
       " 50,\n",
       " 38,\n",
       " 41,\n",
       " 56,\n",
       " 54,\n",
       " 2,\n",
       " 2,\n",
       " 49,\n",
       " 38,\n",
       " 54,\n",
       " 52,\n",
       " 38,\n",
       " 51,\n",
       " 54,\n",
       " 2,\n",
       " 39,\n",
       " 55,\n",
       " 42,\n",
       " 38,\n",
       " 48,\n",
       " 54,\n",
       " 55,\n",
       " 50,\n",
       " 57,\n",
       " 48,\n",
       " 54,\n",
       " 60,\n",
       " 55,\n",
       " 2,\n",
       " 42,\n",
       " 2,\n",
       " 55,\n",
       " 40,\n",
       " 38,\n",
       " 51,\n",
       " 55,\n",
       " 57,\n",
       " 56,\n",
       " 2,\n",
       " 41,\n",
       " 45,\n",
       " 42,\n",
       " 41,\n",
       " 48,\n",
       " 12,\n",
       " 2,\n",
       " 2,\n",
       " 41,\n",
       " 8,\n",
       " 46,\n",
       " 57,\n",
       " 55,\n",
       " 42,\n",
       " 38,\n",
       " 40,\n",
       " 50,\n",
       " 38,\n",
       " 39,\n",
       " 50,\n",
       " 51,\n",
       " 1,\n",
       " 55,\n",
       " 57,\n",
       " 40,\n",
       " 52,\n",
       " 51,\n",
       " 48,\n",
       " 38,\n",
       " 55,\n",
       " 19,\n",
       " 46,\n",
       " 38,\n",
       " 2,\n",
       " 2,\n",
       " 48,\n",
       " 8,\n",
       " 51,\n",
       " 57,\n",
       " 48,\n",
       " 40,\n",
       " 54,\n",
       " 38,\n",
       " 41,\n",
       " 55,\n",
       " 52,\n",
       " 55,\n",
       " 60,\n",
       " 53,\n",
       " 48,\n",
       " 55,\n",
       " 50,\n",
       " 46,\n",
       " 41,\n",
       " 2,\n",
       " 61,\n",
       " 42,\n",
       " 55,\n",
       " 38,\n",
       " 48,\n",
       " 52,\n",
       " 42,\n",
       " 38,\n",
       " 41,\n",
       " 52,\n",
       " 48,\n",
       " 8,\n",
       " 2,\n",
       " 55,\n",
       " 57,\n",
       " 41,\n",
       " 48,\n",
       " 44,\n",
       " 38,\n",
       " 2,\n",
       " 42,\n",
       " 38,\n",
       " 42,\n",
       " 51,\n",
       " 54,\n",
       " 57,\n",
       " 39,\n",
       " 55,\n",
       " 2,\n",
       " 54,\n",
       " 55,\n",
       " 42,\n",
       " 42,\n",
       " 43,\n",
       " 55,\n",
       " 41,\n",
       " 41,\n",
       " 42,\n",
       " 49,\n",
       " 45,\n",
       " 2,\n",
       " 54,\n",
       " 55,\n",
       " 38,\n",
       " 38,\n",
       " 2,\n",
       " 40,\n",
       " 56,\n",
       " 2,\n",
       " 2,\n",
       " 61,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 50,\n",
       " 61,\n",
       " 2,\n",
       " 42,\n",
       " 2,\n",
       " 2,\n",
       " 61,\n",
       " 2,\n",
       " 1,\n",
       " 40,\n",
       " 2,\n",
       " 2,\n",
       " 48,\n",
       " 38,\n",
       " 54,\n",
       " 49,\n",
       " 54,\n",
       " 72,\n",
       " 42,\n",
       " 48,\n",
       " 57,\n",
       " 42,\n",
       " 53,\n",
       " 2,\n",
       " 50,\n",
       " 38,\n",
       " 50,\n",
       " 42,\n",
       " 57,\n",
       " 51,\n",
       " 42,\n",
       " 38,\n",
       " 57,\n",
       " 56,\n",
       " 2,\n",
       " 54,\n",
       " 55,\n",
       " 38,\n",
       " 36,\n",
       " 38,\n",
       " 46,\n",
       " 2,\n",
       " 38,\n",
       " 51,\n",
       " 42,\n",
       " 11,\n",
       " 10,\n",
       " 51,\n",
       " 38,\n",
       " 51,\n",
       " 40,\n",
       " 48,\n",
       " 2,\n",
       " 42,\n",
       " 42,\n",
       " 57,\n",
       " 56,\n",
       " 2,\n",
       " 58,\n",
       " 46,\n",
       " 41,\n",
       " 46,\n",
       " 50,\n",
       " 42,\n",
       " 42,\n",
       " 51,\n",
       " 48,\n",
       " 2,\n",
       " 56,\n",
       " 48,\n",
       " 55,\n",
       " 55,\n",
       " 51,\n",
       " 46,\n",
       " 40,\n",
       " 8,\n",
       " 2,\n",
       " 2,\n",
       " 40,\n",
       " 1,\n",
       " 54,\n",
       " 42,\n",
       " 54,\n",
       " 41,\n",
       " 2,\n",
       " 54,\n",
       " 46,\n",
       " 42,\n",
       " 51,\n",
       " 57,\n",
       " 49,\n",
       " 42,\n",
       " 2,\n",
       " 2,\n",
       " 42,\n",
       " 54,\n",
       " 44,\n",
       " 50,\n",
       " 61,\n",
       " 2,\n",
       " 50,\n",
       " 55,\n",
       " 55,\n",
       " 38,\n",
       " 50,\n",
       " 2,\n",
       " 54,\n",
       " 38,\n",
       " 48,\n",
       " 51,\n",
       " 42,\n",
       " 38,\n",
       " 48,\n",
       " 54,\n",
       " 42,\n",
       " 47,\n",
       " 40,\n",
       " 8,\n",
       " 2,\n",
       " 55,\n",
       " 48,\n",
       " 38,\n",
       " 2,\n",
       " 54,\n",
       " 50,\n",
       " 50,\n",
       " 54,\n",
       " 55,\n",
       " 2,\n",
       " 55,\n",
       " 56,\n",
       " 71,\n",
       " 48,\n",
       " 2,\n",
       " 2,\n",
       " 50,\n",
       " 41,\n",
       " 2,\n",
       " 42,\n",
       " 51,\n",
       " 42,\n",
       " 51,\n",
       " 38,\n",
       " 54,\n",
       " 50,\n",
       " 48,\n",
       " 1,\n",
       " 56,\n",
       " 41,\n",
       " 54,\n",
       " 42,\n",
       " 49,\n",
       " 49,\n",
       " 14,\n",
       " 55,\n",
       " 56,\n",
       " 42,\n",
       " 2,\n",
       " 2,\n",
       " 55,\n",
       " 46,\n",
       " 54,\n",
       " 52,\n",
       " 38,\n",
       " 72,\n",
       " 2,\n",
       " 48,\n",
       " 2,\n",
       " 2,\n",
       " 56,\n",
       " 38,\n",
       " 57,\n",
       " 39,\n",
       " 50,\n",
       " 51,\n",
       " 48,\n",
       " 2,\n",
       " 54,\n",
       " 38,\n",
       " 53,\n",
       " 2,\n",
       " 50,\n",
       " 42,\n",
       " 38,\n",
       " 55,\n",
       " 38,\n",
       " 2,\n",
       " 54,\n",
       " 42,\n",
       " 38,\n",
       " 52,\n",
       " 53,\n",
       " 2,\n",
       " 1,\n",
       " 38,\n",
       " 50,\n",
       " 38,\n",
       " 57,\n",
       " 2,\n",
       " 54,\n",
       " 54,\n",
       " 41,\n",
       " 2,\n",
       " 42,\n",
       " 54,\n",
       " 2,\n",
       " 42,\n",
       " 42,\n",
       " 57,\n",
       " 42,\n",
       " 50,\n",
       " 57,\n",
       " 41,\n",
       " 2,\n",
       " 2,\n",
       " 47,\n",
       " 70,\n",
       " 41,\n",
       " 56,\n",
       " 42,\n",
       " 42,\n",
       " 54,\n",
       " 56,\n",
       " 2,\n",
       " 56,\n",
       " 51,\n",
       " 48,\n",
       " 57,\n",
       " 52,\n",
       " 48,\n",
       " 50,\n",
       " 41,\n",
       " 40,\n",
       " 38,\n",
       " 46,\n",
       " 55,\n",
       " 2,\n",
       " 71,\n",
       " 42,\n",
       " 2,\n",
       " 54,\n",
       " 2,\n",
       " 50,\n",
       " 51,\n",
       " 8,\n",
       " 48,\n",
       " 38,\n",
       " 2,\n",
       " 2,\n",
       " 2,\n",
       " 56,\n",
       " 2,\n",
       " 57,\n",
       " 2,\n",
       " 38,\n",
       " 54,\n",
       " 57,\n",
       " 46,\n",
       " 51,\n",
       " 51,\n",
       " 48,\n",
       " 52,\n",
       " 49,\n",
       " 38,\n",
       " 42,\n",
       " 53,\n",
       " 48,\n",
       " 1,\n",
       " 38,\n",
       " 39,\n",
       " 48,\n",
       " 38,\n",
       " 50,\n",
       " 54,\n",
       " 2,\n",
       " 49,\n",
       " 45,\n",
       " 2,\n",
       " 40,\n",
       " 2,\n",
       " 2,\n",
       " 50,\n",
       " 41,\n",
       " 42,\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c2_dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:29.704854Z",
     "start_time": "2019-01-04T18:52:29.462012Z"
    }
   },
   "outputs": [],
   "source": [
    "x1 = np.stack(c1_dat)\n",
    "x2 = np.stack(c2_dat)\n",
    "x3 = np.stack(c3_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:30.186933Z",
     "start_time": "2019-01-04T18:52:30.182935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2, 46, 54, ...,  2, 55, 42])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:31.070176Z",
     "start_time": "2019-01-04T18:52:30.974229Z"
    }
   },
   "outputs": [],
   "source": [
    "y = np.stack(c4_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 4 inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:31.771621Z",
     "start_time": "2019-01-04T18:52:31.765625Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2, 46, 54, 52]), array([28, 49, 38, 38]), array([54, 42,  2, 54]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1[:4], x2[:4], x3[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:32.211532Z",
     "start_time": "2019-01-04T18:52:32.206535Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([46, 54, 52, 56])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:32.701174Z",
     "start_time": "2019-01-04T18:52:32.697178Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((53333,), (53333,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a size for our hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:33.938049Z",
     "start_time": "2019-01-04T18:52:33.934051Z"
    }
   },
   "outputs": [],
   "source": [
    "n_hidden = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of latent factors to create (i.e. the size of the embedding matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:52:34.968015Z",
     "start_time": "2019-01-04T18:52:34.966014Z"
    }
   },
   "outputs": [],
   "source": [
    "n_fac = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-04T18:53:41.840453Z",
     "start_time": "2019-01-04T18:53:41.774491Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-c242aa642dbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n",
    "print(output.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:15:08.845180Z",
     "start_time": "2019-01-03T11:15:08.701265Z"
    }
   },
   "outputs": [],
   "source": [
    "class Char3Model(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "\n",
    "        # The 'green arrow' from our diagram - the layer operation from input to hidden\n",
    "        self.l_in = nn.Linear(n_fac, n_hidden)\n",
    "\n",
    "        # The 'orange arrow' from our diagram - the layer operation from hidden to hidden\n",
    "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "        # The 'blue arrow' from our diagram - the layer operation from hidden to output\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, c1, c2, c3):\n",
    "        in1 = F.relu(self.l_in(self.e(c1)))\n",
    "        in2 = F.relu(self.l_in(self.e(c2)))\n",
    "        in3 = F.relu(self.l_in(self.e(c3)))\n",
    "        \n",
    "        h = V(torch.zeros(in1.size()))\n",
    "        h = torch.tanh(self.l_hidden(h+in1))\n",
    "        h = torch.tanh(self.l_hidden(h+in2))\n",
    "        h = torch.tanh(self.l_hidden(h+in3))\n",
    "        \n",
    "        return F.log_softmax(self.l_out(h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modelo LSTM propio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:32:43.665450Z",
     "start_time": "2019-01-03T11:32:43.518534Z"
    }
   },
   "outputs": [],
   "source": [
    "class Char3LSTMModel(nn.Module):\n",
    "    def __init__(self,vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.l_in = nn.Linear(n_fac, n_hidden)\n",
    "        \n",
    "        # Matriz de pesos para input gate\n",
    "        self.l_xi = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "        \n",
    "        # Matriz de pesos para forget gate f(t)\n",
    "        self.l_xf = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "        \n",
    "        # Matriz dea  gate g(t)\n",
    "        self.l_xg = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "        \n",
    "        # Matriz de pesos para output gate i(t)\n",
    "        self.l_xo = nn.Linear(n_hidden, n_hidden)\n",
    "        \n",
    "        # The 'blue arrow' from our diagram - the layer operation from hidden to output\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,*cs):\n",
    "        #Embeddings\n",
    "        \n",
    "        #in1 = F.relu(self.l_in(self.e(c1)))\n",
    "        #in2 = F.relu(self.l_in(self.e(c2)))\n",
    "        #in3 = F.relu(self.l_in(self.e(c3)))\n",
    "        #pdb.set_trace()\n",
    "        \n",
    "        bs = cs[0].size()[0]\n",
    "        self.h = V(torch.zeros([bs,n_hidden]))\n",
    "        self.c = V(torch.ones([bs,n_hidden]))\n",
    "         \n",
    "        for car in cs:\n",
    "            \n",
    "            inc = F.relu(self.l_in(self.e(car)))\n",
    "            self.h,self.c = self.cellLSTM(inc,self.h,self.c)\n",
    "            \n",
    "          \n",
    "        return F.log_softmax(self.l_out(self.h))\n",
    "    \n",
    "    def cellLSTM(self,c_emb,h,c):\n",
    "        it = F.logsigmoid(self.l_xi(c_emb+self.h))\n",
    "        ft = F.logsigmoid(self.l_xf(c_emb+self.h))\n",
    "        gt = torch.tanh(self.l_xg(c_emb+self.h))\n",
    "        c = torch.mul(ft,self.c) + torch.mul(it,gt)\n",
    "        self.h =torch.mul(F.logsigmoid(self.l_xo(c_emb+self.h)),torch.tanh(self.c))\n",
    "        return self.h,self.c\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:32:44.349083Z",
     "start_time": "2019-01-03T11:32:44.216123Z"
    }
   },
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_arrays('.', [-1], np.stack([x1,x2,x3], axis=1), y, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:32:45.031670Z",
     "start_time": "2019-01-03T11:32:44.903723Z"
    }
   },
   "outputs": [],
   "source": [
    "a=np.stack([x1,x2,x3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:32:45.813218Z",
     "start_time": "2019-01-03T11:32:45.679308Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53333"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:32:46.669241Z",
     "start_time": "2019-01-03T11:32:46.533320Z"
    }
   },
   "outputs": [],
   "source": [
    "#m = Char3Model(vocab_size, n_fac)\n",
    "m=Char3LSTMModel(vocab_size, n_fac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:32:47.565791Z",
     "start_time": "2019-01-03T11:32:47.246989Z"
    }
   },
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xs,yt = next(it)\n",
    "t = m(*V(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:32:48.300467Z",
     "start_time": "2019-01-03T11:32:48.167546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs[0].size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:32:48.981076Z",
     "start_time": "2019-01-03T11:32:48.852150Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:33:00.585694Z",
     "start_time": "2019-01-03T11:32:49.895864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                                 \n",
      "    0      2.437601   2.199118  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.199118137359619]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:28:32.258226Z",
     "start_time": "2019-01-03T11:28:32.121306Z"
    }
   },
   "outputs": [],
   "source": [
    "set_lrs(opt, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:34:58.273521Z",
     "start_time": "2019-01-03T11:34:58.031090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0.]]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m(T([[41,1,3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:28:44.019681Z",
     "start_time": "2019-01-03T11:28:33.008034Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c0e20513fc472cacd3202e2c2c2e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss                                                 \n",
      "    0      2.152131   1.01025   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0102496147155762]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:30:42.763316Z",
     "start_time": "2019-01-03T11:30:42.623369Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    pdb.set_trace()\n",
    "    idxs = T(np.array([char_indices[c] for c in inp]))\n",
    "    #pdb.set_trace()\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    #i = np.argmax(p.view(-1).detach().numpy())\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-03T11:32:09.203422Z",
     "start_time": "2019-01-03T11:30:45.289354Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-130-225367365578>(3)get_next()\n",
      "-> idxs = T(np.array([char_indices[c] for c in inp]))\n",
      "(Pdb) n\n",
      "> <ipython-input-130-225367365578>(5)get_next()\n",
      "-> p = m(*VV(idxs))\n",
      "(Pdb) idxs\n",
      "tensor([41, 42,  2])\n",
      "(Pdb) V(idxs)\n",
      "tensor([41, 42,  2])\n",
      "(Pdb) VV(idxs)\n",
      "tensor([41, 42,  2])\n",
      "(Pdb) *VV(idxs)\n",
      "*** SyntaxError: can't use starred expression here\n",
      "(Pdb) c\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-131-8a7c3ea61a9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_next\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'de '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-130-225367365578>\u001b[0m in \u001b[0;36mget_next\u001b[1;34m(inp)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0midxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchar_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#pdb.set_trace()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mVV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midxs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_np\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m#i = np.argmax(p.view(-1).detach().numpy())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 489\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    490\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-121-9b302d9ca375>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, *cs)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;31m#pdb.set_trace()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0mbs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mones\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_hidden\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "get_next('de ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T18:31:12.388370Z",
     "start_time": "2018-12-19T18:31:12.104298Z"
    }
   },
   "outputs": [],
   "source": [
    "get_next('err')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-19T18:32:45.607970Z",
     "start_time": "2018-12-19T18:32:45.340907Z"
    }
   },
   "outputs": [],
   "source": [
    "get_next(' de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next('and')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first RNN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the size of our unrolled RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of 0 through 7, create a list of every 8th character with that starting point. These will be the 8 inputs to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(len(idx)-cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_in_dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create a list of the next character in each of these series. This will be the labels for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_out_dat = [idx[j+cs] for j in range(len(idx)-cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_out_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.stack(c_in_dat, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.stack(c_out_dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So each column below is one series of 8 characters from the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs[:cs,:cs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and this is the next character after each sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[:cs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = get_cv_idxs(len(idx)-cs-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_arrays('.', val_idx, xs, y, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLoopModel(nn.Module):\n",
    "    # This is an RNN!\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.l_in = nn.Linear(n_fac, n_hidden)\n",
    "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = V(torch.zeros(bs, n_hidden))\n",
    "        for c in cs:\n",
    "            inp = F.relu(self.l_in(self.e(c)))\n",
    "            h = F.tanh(self.l_hidden(h+inp))\n",
    "        \n",
    "        return F.log_softmax(self.l_out(h), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharLoopModel(vocab_size, n_fac)\n",
    "opt = optim.Adam(m.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLoopConcatModel(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.l_in = nn.Linear(n_fac+n_hidden, n_hidden)\n",
    "        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = V(torch.zeros(bs, n_hidden))\n",
    "        for c in cs:\n",
    "            inp = torch.cat((h, self.e(c)), 1)\n",
    "            inp = F.relu(self.l_in(inp))\n",
    "            h = torch.tanh(self.l_hidden(inp))#   F.tanh(self.l_hidden(inp))\n",
    "            #h = F.tanh(self.l_hidden(inp))\n",
    "        \n",
    "        return F.log_softmax(self.l_out(h), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharLoopConcatModel(vocab_size, n_fac)\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xs,yt = next(it)\n",
    "t = m(*V(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 1, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = T(np.array([char_indices[c] for c in inp]))\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next('en un lu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next('part of ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next('queens a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = V(torch.zeros(1, bs, n_hidden))\n",
    "        inp = self.e(torch.stack(cs))\n",
    "        outp,h = self.rnn(inp, h)\n",
    "        \n",
    "        return F.log_softmax(self.l_out(outp[-1]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharRnn(vocab_size, n_fac)\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xs,yt = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = m.e(V(torch.stack(xs)))\n",
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht = V(torch.zeros(1, 512,n_hidden))\n",
    "outp, hn = m.rnn(t, ht)\n",
    "outp.size(), hn.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = m(*V(xs)); t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 2, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = T(np.array([char_indices[c] for c in inp]))\n",
    "    p = m(*VV(idxs))\n",
    "    i = np.argmax(to_np(p))\n",
    "    return chars[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = get_next(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next_n('for thos', 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-output model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take non-overlapping sets of characters this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_in_dat = [[idx[i+j] for i in range(cs)] for j in range(0, len(idx)-cs-1, cs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then create the exact same thing, offset by 1, as our labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_out_dat = [[idx[i+j] for i in range(cs)] for j in range(1, len(idx)-cs, cs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.stack(c_in_dat)\n",
    "xs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = np.stack(c_out_dat)\n",
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xs[:cs,:cs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys[:cs,:cs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_idx = get_cv_idxs(len(xs)-cs-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ColumnarModelData.from_arrays('.', val_idx, xs, ys, bs=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac):\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        \n",
    "    def forward(self, *cs):\n",
    "        bs = cs[0].size(0)\n",
    "        h = V(torch.zeros(1, bs, n_hidden))\n",
    "        inp = self.e(torch.stack(cs))\n",
    "        outp,h = self.rnn(inp, h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqRnn(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(md.trn_dl)\n",
    "*xst,yt = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_loss_seq(inp, targ):\n",
    "    sl,bs,nh = inp.size()\n",
    "    targ = targ.transpose(0,1).contiguous().view(-1)\n",
    "    return F.nll_loss(inp.view(-1,nh), targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 1, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identity init!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqRnn(vocab_size, n_fac).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.rnn.weight_hh_l0.data.copy_(torch.eye(n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, nll_loss_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stateful model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext import vocab, data\n",
    "\n",
    "from fastai.nlp import *\n",
    "from fastai.lm_rnn import *\n",
    "\n",
    "PATH='data/nietzsche/'\n",
    "\n",
    "TRN_PATH = 'trn/'\n",
    "VAL_PATH = 'val/'\n",
    "TRN = f'{PATH}{TRN_PATH}'\n",
    "VAL = f'{PATH}{VAL_PATH}'\n",
    "\n",
    "# Note: The student needs to practice her shell skills and prepare her own dataset before proceeding:\n",
    "# - trn/trn.txt (first 80% of nietzsche.txt)\n",
    "# - val/val.txt (last 20% of nietzsche.txt)\n",
    "\n",
    "%ls {PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls {PATH}trn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize=list)\n",
    "bs=64; bptt=8; n_fac=42; n_hidden=256\n",
    "\n",
    "FILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\n",
    "md = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n",
    "\n",
    "len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRnn(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        self.vocab_size = vocab_size\n",
    "        super().__init__()\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNN(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulRnn(md.nt, n_fac, 512).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)\n",
    "\n",
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the pytorch source\n",
    "\n",
    "def RNNCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n",
    "    return F.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulRnn2(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.RNNCell(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
    "        outp = []\n",
    "        o = self.h\n",
    "        for c in cs: \n",
    "            o = self.rnn(self.e(c), o)\n",
    "            outp.append(o)\n",
    "        outp = self.l_out(torch.stack(outp))\n",
    "        self.h = repackage_var(o)\n",
    "        return F.log_softmax(outp, dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulRnn2(md.nt, n_fac, 512).cuda()\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 4, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-29T12:44:52.095639Z",
     "start_time": "2018-12-29T12:44:51.942741Z"
    }
   },
   "outputs": [],
   "source": [
    "??nn.LSTMCell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulGRU(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.GRU(n_fac, n_hidden)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h.size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the pytorch source code - for reference\n",
    "\n",
    "def GRUCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n",
    "    gi = F.linear(input, w_ih, b_ih)\n",
    "    gh = F.linear(hidden, w_hh, b_hh)\n",
    "    i_r, i_i, i_n = gi.chunk(3, 1)\n",
    "    h_r, h_i, h_n = gh.chunk(3, 1)\n",
    "\n",
    "    resetgate = F.sigmoid(i_r + h_r)\n",
    "    inputgate = F.sigmoid(i_i + h_i)\n",
    "    newgate = F.tanh(i_n + resetgate * h_n)\n",
    "    return newgate + inputgate * (hidden - newgate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulGRU(md.nt, n_fac, 512).cuda()\n",
    "\n",
    "opt = optim.Adam(m.parameters(), 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 6, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lrs(opt, 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 3, opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together: LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai import sgdr\n",
    "\n",
    "n_hidden=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharSeqStatefulLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, n_fac, bs, nl):\n",
    "        super().__init__()\n",
    "        self.vocab_size,self.nl = vocab_size,nl\n",
    "        self.e = nn.Embedding(vocab_size, n_fac)\n",
    "        self.rnn = nn.LSTM(n_fac, n_hidden, nl, dropout=0.5)\n",
    "        self.l_out = nn.Linear(n_hidden, vocab_size)\n",
    "        self.init_hidden(bs)\n",
    "        \n",
    "    def forward(self, cs):\n",
    "        bs = cs[0].size(0)\n",
    "        if self.h[0].size(1) != bs: self.init_hidden(bs)\n",
    "        outp,h = self.rnn(self.e(cs), self.h)\n",
    "        self.h = repackage_var(h)\n",
    "        return F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n",
    "    \n",
    "    def init_hidden(self, bs):\n",
    "        self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n",
    "                  V(torch.zeros(self.nl, bs, n_hidden)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = CharSeqStatefulLSTM(md.nt, n_fac, 512, 2).cuda()\n",
    "lo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(f'{PATH}models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(m, md, 2, lo.opt, F.nll_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "on_end = lambda sched, cycle: save_model(m, f'{PATH}models/cyc_{cycle}')\n",
    "cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]\n",
    "fit(m, md, 2**6-1, lo.opt, F.nll_loss, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next(inp):\n",
    "    idxs = TEXT.numericalize(inp)\n",
    "    p = m(VV(idxs.transpose(0,1)))\n",
    "    r = torch.multinomial(p[-1].exp(), 1)\n",
    "    return TEXT.vocab.itos[to_np(r)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_next('for thos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_n(inp, n):\n",
    "    res = inp\n",
    "    for i in range(n):\n",
    "        c = get_next(inp)\n",
    "        res += c\n",
    "        inp = inp[1:]+c\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_next_n('for thos', 400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
